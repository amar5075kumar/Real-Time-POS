{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d13780f-c54d-4e9a-b6a8-7faf176802f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Environment Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbf15f7-6849-4be3-b262-3ddb7fdf8b85",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing the libraries"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    " \n",
    "from delta.tables import *\n",
    " \n",
    "import dlt\n",
    "import boto3\n",
    "import socket\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0260c9-83ef-4b56-a2ff-5ada2ed1b97e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Define AWS configuration details in the aws_config dictionary\n",
    "# DBTITLE 1,Initialize Config Settings\n",
    "if 'config' not in locals() or not isinstance(config, dict):\n",
    "    config = {}\n",
    "\n",
    "config['aws'] = {\n",
    "    'access_key_id': 'AKIA3BYT2DKQQQ7QDXVW',\n",
    "    'secret_access_key': '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',\n",
    "    'region_name': 'us-west-2',\n",
    "    'subnets': [\n",
    "        'subnet-0682160c6cf0f7d33',  # SubnetID-1 \n",
    "        'subnet-0ef26ca8237f37fae'   # SubnetID-2\n",
    "    ],\n",
    "    'security_group': 'sg-098d6eca4e4744f3f',  # Security group ID\n",
    "    'cluster_name': 'real-time-pos-msk',  # Unique cluster name\n",
    "    'kafka_version': '2.8.1',\n",
    "    'number_of_broker_nodes': 4,\n",
    "    'instance_type': 'kafka.m5.large',\n",
    "    'cluster_arn': 'arn:aws:kafka:us-west-2:759713897121:cluster/real-time-pos-msk/d494f0b6-6c35-4d6e-ab46-60da511fcafd-11'\n",
    "}\n",
    "\n",
    "## Config Settings for DBFS Mount Point\n",
    "config['dbfs_mount_name'] = f'/mnt/real-time-pos/' \n",
    "\n",
    "# Store the filenames for the data files into Config\n",
    "config['inventory_change_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_store001.txt'\n",
    "config['inventory_change_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_online.txt'\n",
    " \n",
    "# snapshot data files\n",
    "config['inventory_snapshot_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_store001.txt'\n",
    "config['inventory_snapshot_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_online.txt'\n",
    " \n",
    "# static data files\n",
    "config['stores_filename'] = config['dbfs_mount_name'] + '/data-generator/store.txt'\n",
    "config['items_filename'] = config['dbfs_mount_name'] + '/data-generator/item.txt'\n",
    "config['change_types_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_type.txt'\n",
    "\n",
    "# Config Settings for Checkpoint Files\n",
    "config['inventory_snapshot_path'] = config['dbfs_mount_name'] + '/inventory_snapshots/'\n",
    "# Config Settings for DLT Data\n",
    "config['dlt_pipeline'] = config['dbfs_mount_name'] + '/dlt_pipeline_pos'\n",
    "\n",
    "# Identify Database for Data Objects and initialize it\n",
    "database_name = f'pos_dlt'\n",
    "config['database'] = database_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f59a9d-8470-4514-a5fe-2087e20f69e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create The Database"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS pos_dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab29bc6-052c-4ef3-9539-8c6925c83771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT tables wont be run unless the DLT pipeline is initialized. It cant be run in interactive mode .\n",
    "## to test the dataframe use store definition \n",
    "store_schema = StructType([ StructField(\"store_id\", StringType(), True),\n",
    "                            StructField(\"name\", StringType(), True)\n",
    "                            ])\n",
    "df=(\n",
    "    spark.read.csv(config['stores_filename'],header=True, schema=store_schema)\n",
    "  )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f4c15a-e406-4283-9b09-5d5bd2f2b0a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the Static Data to DLT -- > Store Table"
    }
   },
   "outputs": [],
   "source": [
    "## Store Static Data \n",
    "store_schema = StructType([ StructField(\"store_id\", StringType(), True),\n",
    "                            StructField(\"name\", StringType(), True)\n",
    "                            ])\n",
    "\n",
    "## Define the delta live table for the store data\n",
    "@dlt.table(\n",
    "  name='Store',\n",
    "  comment='This table store Static data with individual store id & Name',\n",
    "  table_properties={'quality':'Silver'},\n",
    "  spark_conf={'pipelines.trigger.interval':'10 minutes'}\n",
    ")\n",
    "def store():\n",
    "  df=(\n",
    "    spark.read.csv(config['stores_filename'],header=True, schema=store_schema)\n",
    "  )\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fea7eb0-ee4d-4dbd-8fab-2a5d155ec73c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the Static Item Table  to DLT -> Item"
    }
   },
   "outputs": [],
   "source": [
    "item_schema=StructType([\n",
    "    StructField(\"item_id\", StringType()),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"supplier_id\",IntegerType(),True),\n",
    "    StructField(\"saftey_stock_quantity\",IntegerType(),True)    \n",
    "])\n",
    "\n",
    "## Defining the Item Table\n",
    "@dlt.table(\n",
    "    name='Item',\n",
    "    comment='This table Include all the items with their details . Triggered Every 10 Minutes',\n",
    "    table_properties={'quality':'Silver'},\n",
    "    spark_conf={'pipelines.trigger.interval':'10 minutes'}\n",
    ")\n",
    "def Item():\n",
    "    df=spark.read.csv(\n",
    "        config['items_filename'],header=True,schema=item_schema\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbeff2a-59fe-4622-b269-970c60a2e9d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inventory Change Type DLT Table"
    }
   },
   "outputs": [],
   "source": [
    "change_type_schema = StructType([\n",
    "  StructField('change_type_id', IntegerType()),\n",
    "  StructField('change_type', StringType())\n",
    "  ])\n",
    " \n",
    "@dlt.table(\n",
    "  name = 'inventory_change_type',\n",
    "  comment = 'data mapping change type id values to descriptive strings',\n",
    "  table_properties={'quality':'silver'},\n",
    "  spark_conf={'pipelines.trigger.interval':'10 minutes'}\n",
    ")\n",
    "def inventory_change_type():\n",
    "  return (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        config['change_types_filename'],\n",
    "        header=True,\n",
    "        schema=change_type_schema\n",
    "        )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32f79c0-9c39-422f-bc64-c375809dd0e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Inventory Change Events"
    }
   },
   "outputs": [],
   "source": [
    "## Initialize the boto3 session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=config['aws']['access_key_id'],\n",
    "    aws_secret_access_key=config['aws']['secret_access_key'],\n",
    "    region_name=config['aws']['region_name']\n",
    ")\n",
    "\n",
    "# Create an MSK client\n",
    "msk_client = session.client('kafka')\n",
    "\n",
    "try:\n",
    "    # Get bootstrap brokers\n",
    "    response = msk_client.get_bootstrap_brokers(\n",
    "        ClusterArn=config['aws']['cluster_arn'],\n",
    "    )\n",
    "    bootstrap_servers = response['BootstrapBrokerString']\n",
    "except NoCredentialsError:\n",
    "    print(\"No credentials available. Please check your AWS credentials.\")\n",
    "    exit(1)  # Exit the script if credentials are not available\n",
    "except Exception as e:\n",
    "    print(f\"Error getting bootstrap brokers: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "topics = ['InventorySnapshot', 'ChangeInventoryData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ddab40-c850-4723-b5cd-e3bb06ec31f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Inventory Change Data"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'raw_inventory_change',\n",
    "  comment= 'data representing raw (untransformed) inventory-relevant events originating from the POS',\n",
    "  table_properties={'quality':'bronze'}\n",
    "  )\n",
    "def raw_inventory_change():\n",
    "  return (\n",
    "    spark\n",
    "      .readStream\n",
    "      .format('kafka')\n",
    "      .option('subscribe', 'ChangeInventoryData')\n",
    "      .option('kafka.bootstrap.servers',bootstrap_servers )\n",
    "      .option('kafka.sasl.mechanism', 'PLAIN')\n",
    "    #   .option('kafka.security.protocol', 'SASL_SSL')\n",
    "    #   .option('kafka.sasl.jaas.config', config['eh_sasl'])\n",
    "      .option('kafka.request.timeout.ms', '120000')\n",
    "      .option('kafka.session.timeout.ms', '120000')\n",
    "      .option('failOnDataLoss', 'false')\n",
    "      .option('startingOffsets', 'earliest')\n",
    "      .option('maxOffsetsPerTrigger', '100') # read 100 messages at a time\n",
    "      .load()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2528c97c-c198-42a8-a9e6-07b223930fbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inspect the DLT table Logic"
    }
   },
   "outputs": [],
   "source": [
    "# display(spark\n",
    "#       .readStream\n",
    "#       .format('kafka')\n",
    "#       .option('subscribe', 'ChangeInventoryData')\n",
    "#       .option('kafka.bootstrap.servers',bootstrap_servers )\n",
    "#       .option('kafka.sasl.mechanism', 'PLAIN')\n",
    "#     #   .option('kafka.security.protocol', 'SASL_SSL')\n",
    "#     #   .option('kafka.sasl.jaas.config', config['eh_sasl'])\n",
    "#       .option('kafka.request.timeout.ms', '120000')\n",
    "#       .option('kafka.session.timeout.ms', '120000')\n",
    "#      .option('failOnDataLoss', 'false')\n",
    "#       .option('startingOffsets', 'earliest')\n",
    "#       .option('maxOffsetsPerTrigger', '100') # read 100 messages at a time\n",
    "#      .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c67438-a56f-494c-ac1e-2b4807fe1e8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Transactions Into Structure Field"
    }
   },
   "outputs": [],
   "source": [
    "## Use the raw_inventory_change DLT to create another DLT for conversion \n",
    "# schema of value field\n",
    "value_schema = StructType([\n",
    "  StructField('trans_id', StringType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('change_type_id', IntegerType()),\n",
    "  StructField('items', ArrayType(\n",
    "    StructType([\n",
    "      StructField('item_id', IntegerType()), \n",
    "      StructField('quantity', IntegerType())\n",
    "      ])\n",
    "    ))\n",
    "  ])\n",
    " \n",
    "# define inventory change data\n",
    "@dlt.table(\n",
    "  name = 'inventory_change',\n",
    "  comment = 'data representing item-level inventory changes originating from the POS',\n",
    "  table_properties = {'quality':'silver'}\n",
    ")\n",
    "def inventory_change():\n",
    "  df = (\n",
    "    dlt\n",
    "      .read_stream('raw_inventory_change')\n",
    "      .withColumn('body', f.expr('cast(value as string)')) # convert payload to string\n",
    "      .withColumn('event', f.from_json('body', value_schema)) # parse json string in payload\n",
    "      .select( # extract data from payload json\n",
    "        f.col('event').alias('event'),\n",
    "        f.col('event.trans_id').alias('trans_id'),\n",
    "        f.col('event.store_id').alias('store_id'), \n",
    "        f.col('event.date_time').alias('date_time'), \n",
    "        f.col('event.change_type_id').alias('change_type_id'), \n",
    "        f.explode_outer('event.items').alias('item')     # explode items so that there is now one item per record\n",
    "        )\n",
    "      .withColumn('item_id', f.col('item.item_id'))\n",
    "      .withColumn('quantity', f.col('item.quantity'))\n",
    "      .drop('item')\n",
    "      .withWatermark('date_time', '1 hour') # ignore any data more than 1 hour old flowing into deduplication\n",
    "      .dropDuplicates(['trans_id','item_id'])  # drop duplicates \n",
    "    )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd878d46-5ea6-4aea-9188-2dea0ecca8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inventory Management Process\n",
    "\n",
    "### 1. Periodically Receiving Files of Inventory Count\n",
    "\n",
    "### 2. Receiving the File from S3 Bucket\n",
    "\n",
    "### 3. Maintaining Two Tables:\n",
    "- **Full History of Inventory Snapshot**\n",
    "- **Latest Inventory Snapshot**\n",
    "\n",
    "### 4. Full History Inventory Snapshot:\n",
    "- **Mode:** Append\n",
    "\n",
    "### 5. Latest Inventory Snapshot:\n",
    "- **Mode:** Merge\n",
    "\n",
    "## Inventory Snapshot Files Details:\n",
    "- **Frequency:** Irregular\n",
    "- **Action:** Process it as soon as it lands\n",
    "- **Solution:** Use Autoloader\n",
    "  - **Function:** Listens for incoming files from the storage path\n",
    "  - **Process:** Handles data for any new arriving files as a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc33dc5-87af-4b66-959d-6efa1c284df8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Access Incoming Snapshots"
    }
   },
   "outputs": [],
   "source": [
    "# inventory snapshot data files (one from each store) -> SCHEMA_IS\n",
    "Schema_IS = StructType([\n",
    "  StructField('item_id', IntegerType()),\n",
    "  StructField('employee_id', IntegerType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('quantity', IntegerType())\n",
    "  ])\n",
    "\n",
    "@dlt.table(\n",
    "      name='InventorySnapshot',\n",
    "      comment='data representing inventory snapshots originating from the POS in S3',\n",
    "      table_properties={'quality':'silver'}\n",
    "  )\n",
    "def InventorySnapshot():\n",
    "    return(\n",
    "         spark.readStream\n",
    "            .format('cloudFiles')\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.includeExistingFiles\",'true')\n",
    "            .option(\"header\",\"true\")\n",
    "            .schema(Schema_IS)\n",
    "            .load(config['inventory_snapshot_path'])\n",
    "            .drop('id')\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572d101c-f778-4a46-9e1d-dda7957b5735",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking the Inventory Snapshot files Using Autoloader"
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#          spark.readStream\n",
    "#             .format('cloudFiles')\n",
    "#             .option(\"cloudFiles.format\", \"csv\")\n",
    "#             .option(\"cloudFiles.includeExistingFiles\",'true')\n",
    "#             .option(\"header\",\"true\")\n",
    "#             .schema(Schema_IS)\n",
    "#             .load(config['inventory_snapshot_path'])\n",
    "#             .drop('id')\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e3e935-7fae-4a88-a6da-7121016f2029",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Latest Snapshot Table"
    }
   },
   "outputs": [],
   "source": [
    "## Inventory Snapshot data - All historical Load \n",
    "## Calculate a table for latest snapshot data\n",
    "## Use apply changes method\n",
    "# create dlt table to hold latest inventory snapshot (if it doesn't exist)\n",
    "dlt.create_streaming_table('latestinventorysnapshot')\n",
    " \n",
    "# merge incoming snapshot data with latest\n",
    "dlt.apply_changes( # merge\n",
    "  target = 'latestinventorysnapshot',\n",
    "  source = 'InventorySnapshot',\n",
    "  keys = ['store_id','item_id'], # match source to target records on these keys\n",
    "  sequence_by = 'date_time' # determine latest value by comparing date_time field\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff267abd-c54f-479c-9a93-ef633497fb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#         spark\n",
    "#       .readStream\n",
    "#       .format('kafka')\n",
    "#       .option('subscribe', 'ChangeInventoryData')\n",
    "#       .option('kafka.bootstrap.servers', bootstrap_servers)\n",
    "#       .option('kafka.sasl.mechanism', 'PLAIN')\n",
    "#     #   .option('kafka.security.protocol', 'SASL_SSL')\n",
    "#     #   .option('kafka.sasl.jaas.config', config['eh_sasl'])\n",
    "#       .option('kafka.request.timeout.ms', '120000')\n",
    "#       .option('kafka.session.timeout.ms', '120000')\n",
    "#       .option('failOnDataLoss', 'false')\n",
    "#       .option('startingOffsets', 'latest')\n",
    "#       .option('maxOffsetsPerTrigger', '1000') # read 1000 messages at a time\n",
    "#       .load()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50aa0c2f-6b6f-4575-b45f-261ede17446d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver To Gold Layer"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(\n",
    "  name='current_inventory',\n",
    "  comment='current inventory count for a product in a store location',\n",
    "  table_properties={'quality':'gold'},\n",
    "  spark_conf={'pipelines.trigger.interval': '5 minutes'}\n",
    ")\n",
    "def current_inventory():\n",
    "\n",
    "    # calculate inventory change with bopis corrections\n",
    "    inventory_change_df = (\n",
    "        dlt\n",
    "        .readStream('inventory_change').alias('x')\n",
    "        .join(\n",
    "            dlt.readStream('Store').alias('y'), \n",
    "            on='store_id'\n",
    "        )\n",
    "        .join(\n",
    "            dlt.readStream('inventory_change_type').alias('z'), \n",
    "            on='change_type_id'\n",
    "        )\n",
    "        .filter(expr(\"NOT(y.name='online' AND z.change_type='bopis')\"))\n",
    "        .select('store_id','item_id','date_time','quantity')\n",
    "    )\n",
    "\n",
    "    # calculate current inventory\n",
    "    inventory_current_df = (\n",
    "        dlt\n",
    "            .readStream('latestinventorysnapshot').alias('a')\n",
    "            .join(\n",
    "            inventory_change_df.alias('b'), \n",
    "            on=expr('''\n",
    "                a.store_id=b.store_id AND \n",
    "                a.item_id=b.item_id AND \n",
    "                a.date_time<=b.date_time\n",
    "                '''), \n",
    "            how='leftouter'\n",
    "            )\n",
    "            .groupBy('a.store_id','a.item_id')\n",
    "            .agg(\n",
    "                first('a.quantity').alias('snapshot_quantity'),\n",
    "                sum('b.quantity').alias('change_quantity'),\n",
    "                first('a.date_time').alias('snapshot_datetime'),\n",
    "                max('b.date_time').alias('change_datetime')\n",
    "                )\n",
    "            .withColumn('change_quantity', coalesce('change_quantity', lit(0)))\n",
    "            .withColumn('current_quantity', expr('snapshot_quantity + change_quantity'))\n",
    "            .withColumn('date_time',expr('GREATEST(snapshot_datetime, change_datetime)'))\n",
    "            .drop('snapshot_datetime','change_datetime')\n",
    "            .orderBy('current_quantity')\n",
    "    )\n",
    "\n",
    "    return inventory_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cde7a8-eeb8-45aa-862b-d9470ff4c734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class InventoryTestSuite:\n",
    "    def setup(self):\n",
    "        # Create the necessary tables and insert test data\n",
    "        spark.sql(\"DROP TABLE IF EXISTS inventory_change\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS Store\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS inventory_change_type\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS latestinventorysnapshot\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE inventory_change (\n",
    "            store_id INT,\n",
    "            item_id INT,\n",
    "            date_time TIMESTAMP,\n",
    "            quantity INT\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        INSERT INTO inventory_change VALUES\n",
    "        (1, 101, '2023-10-01 10:00:00', 10),\n",
    "        (1, 102, '2023-10-01 11:00:00', 5),\n",
    "        (2, 101, '2023-10-01 12:00:00', -3)\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE Store (\n",
    "            store_id INT,\n",
    "            name STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        INSERT INTO Store VALUES\n",
    "        (1, 'physical'),\n",
    "        (2, 'online')\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE inventory_change_type (\n",
    "            change_type_id INT,\n",
    "            change_type STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        INSERT INTO inventory_change_type VALUES\n",
    "        (1, 'sale'),\n",
    "        (2, 'bopis')\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        CREATE TABLE latestinventorysnapshot (\n",
    "            store_id INT,\n",
    "            item_id INT,\n",
    "            date_time TIMESTAMP,\n",
    "            quantity INT\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        INSERT INTO latestinventorysnapshot VALUES\n",
    "        (1, 101, '2023-09-30 10:00:00', 100),\n",
    "        (1, 102, '2023-09-30 11:00:00', 50),\n",
    "        (2, 101, '2023-09-30 12:00:00', 30)\n",
    "        \"\"\")\n",
    "\n",
    "    def test_inventory_logic(self):\n",
    "        from pyspark.sql.functions import expr, col, first, sum, max, coalesce, lit\n",
    "\n",
    "        # Read the tables\n",
    "        inventory_change_df = spark.read.table('inventory_change')\n",
    "        store_df = spark.read.table('Store')\n",
    "        inventory_change_type_df = spark.read.table('inventory_change_type')\n",
    "        latest_inventory_snapshot_df = spark.read.table('latestinventorysnapshot')\n",
    "\n",
    "        # Perform the joins and transformations as defined in the DLT pipeline\n",
    "        inventory_change_df = (\n",
    "            inventory_change_df.alias('x')\n",
    "            .join(store_df.alias('y'), on='store_id')\n",
    "            .join(inventory_change_type_df.alias('z'), on=col('change_type_id') == col('z.change_type_id'))\n",
    "            .filter(expr(\"NOT(y.name='online' AND z.change_type='bopis')\"))\n",
    "            .select('store_id', 'item_id', 'date_time', 'quantity', 'change_type_id')\n",
    "        )\n",
    "\n",
    "        inventory_current_df = (\n",
    "            latest_inventory_snapshot_df.alias('a')\n",
    "            .join(\n",
    "                inventory_change_df.alias('b'),\n",
    "                on=expr('''\n",
    "                    a.store_id=b.store_id AND \n",
    "                    a.item_id=b.item_id AND \n",
    "                    a.date_time<=b.date_time\n",
    "                '''),\n",
    "                how='leftouter'\n",
    "            )\n",
    "            .groupBy('a.store_id', 'a.item_id')\n",
    "            .agg(\n",
    "                first('a.quantity').alias('snapshot_quantity'),\n",
    "                sum('b.quantity').alias('change_quantity'),\n",
    "                first('a.date_time').alias('snapshot_datetime'),\n",
    "                max('b.date_time').alias('change_datetime')\n",
    "            )\n",
    "            .withColumn('change_quantity', coalesce('change_quantity', lit(0)))\n",
    "            .withColumn('current_quantity', expr('snapshot_quantity + change_quantity'))\n",
    "            .withColumn('date_time', expr('GREATEST(snapshot_datetime, change_datetime)'))\n",
    "            .drop('snapshot_datetime', 'change_datetime')\n",
    "            .orderBy('current_quantity')\n",
    "        )\n",
    "\n",
    "        # Display the final DataFrame\n",
    "        display(inventory_current_df)\n",
    "        print(\"Test inventory logic successfully done\")\n",
    "\n",
    "    def teardown(self):\n",
    "        # Drop the tables\n",
    "        spark.sql(\"DROP TABLE IF EXISTS inventory_change\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS Store\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS inventory_change_type\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS latestinventorysnapshot\")\n",
    "        print(\"Teardown successfully done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cacd871-b3c8-4d92-ba01-2a5b91b02328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the test suite\n",
    "test_suite = InventoryTestSuite()\n",
    "test_suite.setup()\n",
    "test_suite.test_inventory_logic()\n",
    "test_suite.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5b0882-79b0-4bc4-ab69-e0c0fa97879d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7978532169318254,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data-Consumer(Bronze-To-Silver)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
