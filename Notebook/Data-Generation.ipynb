{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67954e3a-72ad-4f85-b8f1-5b92aee4c202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Environment Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba713f9-b15d-4695-955d-a8151205bdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define AWS configuration details in the aws_config dictionary\n",
    "# DBTITLE 1,Initialize Config Settings\n",
    "if 'config' not in locals() or not isinstance(config, dict):\n",
    "    config = {}\n",
    "\n",
    "config['aws'] = {\n",
    "    'access_key_id': '***************************',\n",
    "    'secret_access_key': '*****************************',\n",
    "    'region_name': 'us-west-2',\n",
    "    'subnets': [\n",
    "        'subnet-**************',  # SubnetID-1 \n",
    "        'subnet-**************'   # SubnetID-2\n",
    "    ],\n",
    "    'security_group': 'sg-********************',  # Security group ID\n",
    "    'cluster_name': 'real-time-pos-msk',  # Unique cluster name\n",
    "    'kafka_version': '2.8.1',\n",
    "    'number_of_broker_nodes': 4,\n",
    "    'instance_type': 'kafka.m5.large',\n",
    "    'cluster_arn': 'arn:aws:kafka:us-west-2:*****************:cluster/real-time-pos-msk/******************************************'\n",
    "}\n",
    "\n",
    "## Config Settings for DBFS Mount Point\n",
    "config['dbfs_mount_name'] = f'/mnt/real-time-pos/' \n",
    "\n",
    "# Store the filenames for the data files into Config\n",
    "config['inventory_change_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_store001.txt'\n",
    "config['inventory_change_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_online.txt'\n",
    " \n",
    "# snapshot data files\n",
    "config['inventory_snapshot_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_store001.txt'\n",
    "config['inventory_snapshot_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_online.txt'\n",
    " \n",
    "# static data files\n",
    "config['stores_filename'] = config['dbfs_mount_name'] + '/data-generator/store.txt'\n",
    "config['items_filename'] = config['dbfs_mount_name'] + '/data-generator/item.txt'\n",
    "config['change_types_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_type.txt'\n",
    "\n",
    "# Config Settings for Checkpoint Files\n",
    "config['inventory_snapshot_path'] = config['dbfs_mount_name'] + '/inventory_snapshots/'\n",
    "# Config Settings for DLT Data\n",
    "config['dlt_pipeline'] = config['dbfs_mount_name'] + '/dlt_pipeline_pos'\n",
    "\n",
    "# Identify Database for Data Objects and initialize it\n",
    "database_name = f'pos_dlt'\n",
    "config['database'] = database_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5c4c67-9dc0-4c27-a5da-2f7165d72849",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing the Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import datetime, time\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c09adbe-db84-489d-accd-329e33539535",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reset"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the DLT ENV\n",
    "dbutils.fs.rm(config['dlt_pipeline'],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f4acb3-ddb9-48fc-b643-304df228c541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Assemble Inventory Change Records\n",
    "#### - Represent events impacting inventory in a store location.\n",
    "#### - Events include sales transactions, loss, damage, theft, replenishment, and BOPIS.\n",
    "#### - Consolidated into a single stream of inventory change event records.\n",
    "#### - Each event type identified by a change type identifier.\n",
    "#### - Events may involve multiple products (items).\n",
    "#### - Group data by transaction ID for efficient transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2688bdc8-4b57-4396-8891-9a326478f65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define the schema for Inventory Change Data\n",
    "Schema_IC = StructType([\n",
    "    StructField('trans_id', StringType()),  # Transaction ID\n",
    "    StructField('item_id', IntegerType()),  # Item ID\n",
    "    StructField('store_id', IntegerType()),  # Store ID\n",
    "    StructField('date_time', TimestampType()),  # Date and Time of the transaction\n",
    "    StructField('quantity', IntegerType()),  # Quantity of the item\n",
    "    StructField('change_type_id', IntegerType())  # Type of change (e.g., addition, removal)\n",
    "])\n",
    "\n",
    "# List of inventory change data files from different stores\n",
    "inventory_change_files = [\n",
    "  config['inventory_change_store001_filename'],  # Inventory change file for store 001\n",
    "  config['inventory_change_online_filename']  # Inventory change file for online store\n",
    "]\n",
    "\n",
    "## Read the Inventory Change data using Spark DataFrame with fixed Schema\n",
    "change_inventory = (\n",
    "spark\n",
    "    .read\n",
    "    .csv(\n",
    "      inventory_change_files, \n",
    "      header=True,  # First row contains header\n",
    "      schema=Schema_IC,  # Use the predefined schema\n",
    "      timestampFormat='yyyy-MM-dd HH:mm:ss'  # Timestamp format in the data\n",
    "      )\n",
    "        .withColumn('trans_id', f.expr(\"substring(trans_id, 2, length(trans_id)-2)\"))  # Clean up transaction ID\n",
    "        .withColumn('item', f.struct('item_id', 'quantity'))  # Create a struct column for item_id and quantity\n",
    "        .groupBy('date_time', 'trans_id')  # Group by date_time and transaction ID\n",
    "        .agg(\n",
    "            f.first('store_id').alias('store_id'),  # Get the first store_id in the group\n",
    "            f.first('change_type_id').alias('change_type_id'),  # Get the first change_type_id in the group\n",
    "            f.collect_list('item').alias('items')  # Collect all items in the group into a list\n",
    "        )\n",
    "        .orderBy('date_time', 'trans_id')  # Order by date_time and transaction ID\n",
    "        .toJSON()  # Convert the DataFrame to JSON format\n",
    "        .collect()  # Collect the result as a list\n",
    ")\n",
    "\n",
    "display(spark.read.json(sc.parallelize(change_inventory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e090ff4b-0bae-4200-9ead-e2c3a838e722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Evaluate the Spark DataFrame\n",
    "eval(change_inventory[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b45d53a1-1e36-4fba-96b4-b2222db1a111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Assemble Inventory Snapshots\n",
    "##### Inventory snapshots represent the physical counts taken of products sitting in inventory.\n",
    "##### Such counts are periodically taken to capture the true state of a store's inventory and are necessary given the challenges most retailers encounter in inventory management.\n",
    "\n",
    "##### With each snapshot, we capture basic information about the store, item, and quantity on-hand along with the date time and employee associated with the count. So that the impact of inventory snapshots may be more rapidly reflected in our streams, we simulate a complete recount of products in a store every 5-days. This is far more aggressive than would occur in the real world but again helps to demonstrate the streaming logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfebef3f-77e6-4ca6-87bd-43cdd0e8e896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# format of inventory snapshot records\n",
    "# inventory snapshot data files (one from each store) -> SCHEMA_IS\n",
    "Schema_IS = StructType([\n",
    "  StructField('item_id', IntegerType()),\n",
    "  StructField('employee_id', IntegerType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('quantity', IntegerType())\n",
    "  ])\n",
    " \n",
    "# inventory snapshot files\n",
    "inventory_snapshot_files = [ \n",
    "  config['inventory_snapshot_store001_filename'],\n",
    "  config['inventory_snapshot_online_filename']\n",
    "  ]\n",
    " \n",
    "# read inventory snapshot data\n",
    "snapshots_inventory = (\n",
    "  spark\n",
    "    .read\n",
    "    .csv(\n",
    "      inventory_snapshot_files, \n",
    "      header=True, \n",
    "      timestampFormat='yyyy-MM-dd HH:mm:ss', \n",
    "      schema=Schema_IS\n",
    "      )\n",
    "  )\n",
    " \n",
    "display(snapshots_inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51a8a63-b544-41e1-a137-53b81f6adce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval(snapshots_inventory.toJSON().collect()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a48e9ba-e6f8-4dc5-922a-88171d440f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get date_time of each inventory snapshot by store\n",
    "inventory_snapshot_times = (\n",
    "  snapshots_inventory\n",
    "    .select('date_time','store_id')\n",
    "    .distinct()\n",
    "    .orderBy('date_time')  # sorting of list is essential for logic below\n",
    "  )\n",
    " \n",
    "# display snapshot times\n",
    "display(inventory_snapshot_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e205ae-4573-4f54-b202-f2de569f41f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get date_time of each inventory snapshot by store\n",
    "inventory_snapshot_times = (\n",
    "  snapshots_inventory\n",
    "    .select('date_time','store_id')\n",
    "    .distinct()\n",
    "    .orderBy('date_time')  # sorting of list is essential for logic below\n",
    "  )\n",
    " \n",
    "# display snapshot times\n",
    "display(inventory_snapshot_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47622f30-7786-4208-bfa7-edfef11b2577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Send the Inventory Snapshot and Change Inventory Data to Different topics on MSK cluster \n",
    "##### Create two topic for and send the data to producer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c45e51c-9dd6-46c4-940c-169f9b99ad09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Initialize the boto3 session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=config['aws']['access_key_id'],\n",
    "    aws_secret_access_key=config['aws']['secret_access_key'],\n",
    "    region_name=config['aws']['region_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd20ce6-408c-4532-b64a-fb1d71f6aa70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing the topic"
    }
   },
   "outputs": [],
   "source": [
    "# Create an MSK client\n",
    "msk_client = session.client('kafka')\n",
    "\n",
    "try:\n",
    "    # Get bootstrap brokers\n",
    "    response = msk_client.get_bootstrap_brokers(\n",
    "        ClusterArn=config['aws']['cluster_arn'],\n",
    "    )\n",
    "    bootstrap_servers = response['BootstrapBrokerString']\n",
    "except NoCredentialsError:\n",
    "    print(\"No credentials available. Please check your AWS credentials.\")\n",
    "    exit(1)  # Exit the script if credentials are not available\n",
    "except Exception as e:\n",
    "    print(f\"Error getting bootstrap brokers: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Kafka Admin Client operations\n",
    "try:\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        security_protocol='PLAINTEXT',  \n",
    "        client_id=socket.gethostname(),\n",
    "    )\n",
    "    existing_topics = admin_client.list_topics()\n",
    "    # Create topics\n",
    "    topics = ['InventorySnapshot', 'ChangeInventoryData']\n",
    "    new_topics = [NewTopic(name=topic, num_partitions=1, replication_factor=1) for topic in topics if topic not in existing_topics]\n",
    "    \n",
    "    if new_topics:\n",
    "        admin_client.create_topics(new_topics=new_topics, validate_only=False)\n",
    "        print(f'Created New Topic: {new_topics}')\n",
    "    else:\n",
    "        print(f'Topics already exist')\n",
    "        \n",
    "except NoBrokersAvailable:\n",
    "    print(\"No brokers available. Please check the broker endpoint and network connectivity.\")\n",
    "except KafkaTimeoutError:\n",
    "    print(\"Kafka timeout error. Failed to update metadata after 60.0 secs. Please check the broker endpoint and network connectivity.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # List topics and close the admin client to release resources\n",
    "    if 'admin_client' in locals():  # Check if admin_client is defined\n",
    "        try:\n",
    "            topics = admin_client.list_topics()\n",
    "            print(\"Topics in the cluster:\", topics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing topics: {e}\")\n",
    "        admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80154418-5b32-4402-b880-9876e0a1b3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Sendind data to KAFKA and S3 bucket\n",
    "1. **Random Sampling of Change Inventory Data**\n",
    "   - Randomly sample 100 records from the `change_inventory` dataset.\n",
    "\n",
    "2. **Event Speed Factor and Message Size Configuration**\n",
    "   - Set the event speed factor to 10x real-time speed.\n",
    "   - Define the maximum message size as 256KB.\n",
    "\n",
    "3. **Iterate Through Sampled Events**\n",
    "   - For each event in the sampled data:\n",
    "     - Extract the datetime from the transaction document.\n",
    "     - Compare the event datetime with snapshot datetimes.\n",
    "\n",
    "4. **Inventory Snapshot Transmission**\n",
    "   - For each snapshot datetime that the event datetime exceeds:\n",
    "     - Extract snapshot data for the specific datetime and store ID.\n",
    "     - Transmit the snapshot data to S3 as a CSV file.\n",
    "     - Remove the snapshot datetime from the list after transmission.\n",
    "\n",
    "5. **Calculate Delay Between Events**\n",
    "   - Calculate the delay between the current event and the previous event.\n",
    "   - Adjust the delay by the event speed factor.\n",
    "\n",
    "6. **Transmit Inventory Change Event**\n",
    "   - If the event size exceeds the maximum message size:\n",
    "     - Split the items in the event and send them individually to Kafka.\n",
    "     - Pause transmission every 25 items to avoid overwhelming MSK.\n",
    "   - Otherwise, send the entire transaction document to Kafka.\n",
    "\n",
    "7. **Update Last Event Datetime**\n",
    "   - Set the last event datetime for the next iteration.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2188f93d-7d8b-4d4e-a2e5-c498fc2fcac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Kafka producer\n",
    "producer= KafkaProducer(\n",
    "   bootstrap_servers=bootstrap_servers,\n",
    "   value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Function to send messages to Kafka\n",
    "def send_to_kafka(producer, topic, message):\n",
    "    producer.send(topic, value=message)\n",
    "    producer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89348a3a-b887-480a-94b3-a210546b04d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Randomly sample change inventory data\n",
    "change_inventory_sample = random.sample(change_inventory, 100)\n",
    "\n",
    "max_msg_size = 256 * 1024  # event message to MSK cannot exceed 256KB\n",
    "\n",
    "for event in change_inventory_sample:\n",
    "    # extract datetime from transaction document\n",
    "    d = json.loads(event)  # parse json as a dictionary\n",
    "    dt = datetime.datetime.strptime(d['date_time'], '%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "    # inventory snapshot transmission\n",
    "    # -----------------------------------------------------------------------\n",
    "    snapshot_start = time.time()\n",
    "\n",
    "    inventory_snapshot_times_for_loop = inventory_snapshot_times.collect()  # copy snapshot times list as this may be modified in loop\n",
    "    for snapshot_dt, store_id in inventory_snapshot_times_for_loop:  # for each snapshot\n",
    "\n",
    "        # extract snapshot data for this dt\n",
    "        snapshot_pd = (\n",
    "            snapshots_inventory\n",
    "            .filter(f.expr(\"store_id='{}' AND date_time='{}'\".format(store_id, snapshot_dt)))\n",
    "            .withColumn('date_time', f.expr(\"date_format(date_time, 'yyyy-MM-dd HH:mm:ss')\"))  # force timestamp conversion to include\n",
    "            .toPandas()\n",
    "        )\n",
    "\n",
    "        # transmit to S3 as csv\n",
    "        s3_client = session.client('s3')\n",
    "        s3_client.put_object(\n",
    "            Bucket=\"real-time-pos-msk\",\n",
    "            Key=(\"inventory_snapshots/\" +\n",
    "                    'inventory_snapshot_{}_{}.csv'.format(store_id, snapshot_dt.strftime('%Y-%m-%d_%H-%M-%S'))),\n",
    "            Body=snapshot_pd.to_csv(index=False, mode='a', header=False)\n",
    "        )\n",
    "\n",
    "        # Print confirmation of file sent to S3\n",
    "        print(f'Sent to S3: inventory_snapshot_{store_id}_{snapshot_dt.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')\n",
    "\n",
    "        # remove snapshot date from inventory_snapshot_times\n",
    "        inventory_snapshot_times = inventory_snapshot_times.filter(inventory_snapshot_times['date_time'] != snapshot_dt)\n",
    "        print('Loaded inventory snapshot for {}'.format(snapshot_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "    snapshot_seconds = time.time() - snapshot_start\n",
    "    # -----------------------------------------------------------------------\n",
    "\n",
    "    # inventory change event transmission\n",
    "    # -----------------------------------------------------------------------\n",
    "    # sleep for 60 seconds\n",
    "    time.sleep(1)\n",
    "\n",
    "    # send items individually if json document too large\n",
    "    if len(event) > max_msg_size:\n",
    "        items = d.pop('items')  # retrieve items collection\n",
    "        for i, item in enumerate(items):  # for each item\n",
    "            temp = [item]\n",
    "            d['items'] = temp  # add a one-item items-collection\n",
    "            send_to_kafka(producer, 'ChangeInventoryData', d)  # send message\n",
    "\n",
    "            # Print confirmation of message sent to Kafka\n",
    "            print(f'Sent to Kafka: {json.dumps(d)}')\n",
    "\n",
    "            if (i + 1) % 25 == 0:  # pause transmission every Xth item to avoid overwhelming MSK\n",
    "                time.sleep(1)\n",
    "    else:  # send whole transaction document\n",
    "        send_to_kafka(producer, 'ChangeInventoryData', d)\n",
    "\n",
    "        # Print confirmation of message sent to Kafka\n",
    "        print(f'Sent to Kafka: {json.dumps(d)}')\n",
    "\n",
    "    # -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268f7a96-4d03-4c1d-ae37-7c1300640bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from kafka import KafkaConsumer\n",
    "# import json\n",
    "\n",
    "# # Initialize Kafka consumer\n",
    "# consumer = KafkaConsumer(\n",
    "#     'ChangeInventoryData',\n",
    "#     bootstrap_servers=bootstrap_servers,\n",
    "#     auto_offset_reset='latest',\n",
    "#     enable_auto_commit=True,\n",
    "#     group_id='my-group',\n",
    "#     value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "# )\n",
    "\n",
    "# # Function to consume data from Kafka topic and display it\n",
    "# def consume_from_kafka(consumer):\n",
    "#     for message in consumer:\n",
    "#         data = message.value\n",
    "#         display(data)\n",
    "\n",
    "# # Start consuming data\n",
    "# consume_from_kafka(consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2485b3e5-bed3-49ad-9883-506377bb84ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2439229363006202,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data-Generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
