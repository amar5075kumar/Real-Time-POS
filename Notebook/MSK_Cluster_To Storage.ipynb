{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9672b78-e255-4743-9c8c-fe40cb9ea231",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment Setup"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Environment Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df72644-019e-4425-a523-39a3398f9e4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from delta.tables import *\n",
    " \n",
    "import dlt as dlt\n",
    "import boto3\n",
    "import socket\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "import boto3\n",
    "import socket\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from kafka.errors import NoBrokersAvailable, KafkaTimeoutError\n",
    "from kafka import KafkaConsumer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ebf2ba-74d9-44c4-990f-ada7c4a52517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define AWS configuration details in the aws_config dictionary\n",
    "# DBTITLE 1,Initialize Config Settings\n",
    "if 'config' not in locals() or not isinstance(config, dict):\n",
    "    config = {}\n",
    "\n",
    "config['aws'] = {\n",
    "    'access_key_id': '***************************',\n",
    "    'secret_access_key': '*****************************',\n",
    "    'region_name': 'us-west-2',\n",
    "    'subnets': [\n",
    "        'subnet-**************',  # SubnetID-1 \n",
    "        'subnet-**************'   # SubnetID-2\n",
    "    ],\n",
    "    'security_group': 'sg-********************',  # Security group ID\n",
    "    'cluster_name': 'real-time-pos-msk',  # Unique cluster name\n",
    "    'kafka_version': '2.8.1',\n",
    "    'number_of_broker_nodes': 4,\n",
    "    'instance_type': 'kafka.m5.large',\n",
    "    'cluster_arn': 'arn:aws:kafka:us-west-2:*****************:cluster/real-time-pos-msk/******************************************'\n",
    "}\n",
    "\n",
    "## Config Settings for DBFS Mount Point\n",
    "config['dbfs_mount_name'] = f'/mnt/real-time-pos/' \n",
    "\n",
    "# Store the filenames for the data files into Config\n",
    "config['inventory_change_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_store001.txt'\n",
    "config['inventory_change_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_online.txt'\n",
    " \n",
    "# snapshot data files\n",
    "config['inventory_snapshot_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_store001.txt'\n",
    "config['inventory_snapshot_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_online.txt'\n",
    " \n",
    "# static data files\n",
    "config['stores_filename'] = config['dbfs_mount_name'] + '/data-generator/store.txt'\n",
    "config['items_filename'] = config['dbfs_mount_name'] + '/data-generator/item.txt'\n",
    "config['change_types_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_type.txt'\n",
    "\n",
    "# Config Settings for Checkpoint Files\n",
    "config['inventory_snapshot_path'] = config['dbfs_mount_name'] + '/inventory_snapshots/'\n",
    "# Config Settings for DLT Data\n",
    "config['dlt_pipeline'] = config['dbfs_mount_name'] + '/dlt_pipeline_pos'\n",
    "\n",
    "# Identify Database for Data Objects and initialize it\n",
    "database_name = f'pos_dlt'\n",
    "config['database'] = database_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b342549-d9ee-46f2-8063-ec40edcb1fb8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Consumer Group"
    }
   },
   "outputs": [],
   "source": [
    "## Create A Consumer to Consume the Data from the Kafka Topic and then write it to the Cloud Storage as CSV New File for \n",
    "## 1 minute data \n",
    "\n",
    "def consume_kafka_topic(topic_name, kafka_bootstrap_servers, checkpoint_path, output_path, aws_access_key_id, aws_secret_access_key):\n",
    "    try:\n",
    "        df = (spark.readStream\n",
    "              .format(\"kafka\")\n",
    "              .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "              .option(\"subscribe\", topic_name)\n",
    "              .option(\"startingOffsets\", \"Earliest\")\n",
    "              .load())\n",
    "\n",
    "        df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "        query = (df.writeStream\n",
    "                 .format(\"csv\")\n",
    "                 .option(\"path\", output_path)\n",
    "                 .option(\"checkpointLocation\", checkpoint_path)\n",
    "                 .option(\"fs.s3a.access.key\", aws_access_key_id)\n",
    "                 .option(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
    "                 .trigger(processingTime='1 minute')\n",
    "                 .start())\n",
    "\n",
    "        query.awaitTermination()\n",
    "    except Exception as e:\n",
    "        print(f\"Error consuming Kafka topic: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e55a163-4450-4981-9513-b2ac31f5bdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a session using Amazon MSK\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "# Create an MSK client\n",
    "msk_client = session.client('kafka')\n",
    "\n",
    "try:\n",
    "    # Get bootstrap brokers\n",
    "    response = msk_client.get_bootstrap_brokers(\n",
    "        ClusterArn=CLUSTER_ARN,\n",
    "    )\n",
    "    bootstrap_servers = response['BootstrapBrokerString']\n",
    "except NoCredentialsError:\n",
    "    print(\"No credentials available. Please check your AWS credentials.\")\n",
    "    exit(1)  # Exit the script if credentials are not available\n",
    "except Exception as e:\n",
    "    print(f\"Error getting bootstrap brokers: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c54ba70-eb28-4650-8699-f858aad894b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bootstrap_server=bootstrap_servers\n",
    "topic_name = 'InventorySnapshot'\n",
    "output_path=f\"s3a://real-time-pos-msk/inventory_snapshots/\"\n",
    "checkpoint_path=config['inventory_snapshot_path']\n",
    "consume_kafka_topic(topic_name, bootstrap_server, checkpoint_path, output_path, aws_access_key_id, aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d527d5ae-cda8-4a15-b31c-fa2aa6ddaf2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7730405528353671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MSK_Cluster_To Storage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
