{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397b60bf-ea57-449f-93c3-c0cc30f87324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup the AWS Environment \n",
    "### We need to AWS Environment for Storage Purpose and Using AWS Managed KAFKA Version \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c27a6d-53d2-42ae-8f7a-cebe55a45e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A. Setting up the Amazon MSK(Managed Streaming for KAKFA)\n",
    "\n",
    "1. Ensure AWS credentials are securely stored and accessed, It should have the proper permission to access and create MSK Cluster.\n",
    "2. Validate the provided subnet IDs and security group ID to ensure they exist and are correct .\n",
    "    -- Use the Same Subnet and Security group ID as that of workspace . It make things lot easier \n",
    "3. Handle potential exceptions beyond NoCredentialsError, such as boto3.exceptions.Boto3Error.\n",
    "4. Log responses and errors appropriately for monitoring and debugging.\n",
    "5. Ensure the Kafka cluster configuration (e.g., KafkaVersion, InstanceType)...\n",
    "6. Consider using environment variables or a secrets manager for sensitive information.\n",
    "7. Verify the region_name is correct and matches the intended AWS region for deployment.\n",
    "8. Ensure the number of broker nodes is appropriate for the expected load and redundancy requirements.\n",
    "    -- It should be in multiple of availability zone used in Subnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5016e5-d90c-49f9-9633-8a713e0ddd06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the kafka-python library for Kafka client\n",
    "%pip install kafka-python\n",
    "\n",
    "# Install the AWS MSK IAM SASL Signer for authentication with AWS MSK\n",
    "%pip install aws-msk-iam-sasl-signer-python\n",
    "\n",
    "# Install the Databricks Delta Live Tables library\n",
    "%pip install dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ea5a3f-7a58-48c1-aba4-f7833b6a91f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration Code Explanation\n",
    "\n",
    "The following section provides a detailed explanation of the configuration code used for setting up and managing the Amazon MSK (Managed Streaming for Kafka) environment. This documentation is intended to help you understand the purpose and functionality of each part of the code, ensuring proper setup and maintenance of the MSK cluster.\n",
    "\n",
    "1. **AWS Credentials**: Ensure that AWS credentials are securely stored and accessed. They should have the necessary permissions to create and manage the MSK cluster.\n",
    "2. **Subnet and Security Group Validation**: Validate the provided subnet IDs and security group ID to ensure they exist and are correct. Using the same subnet and security group ID as the workspace simplifies the setup.\n",
    "3. **Exception Handling**: Handle potential exceptions beyond `NoCredentialsError`, such as `boto3.exceptions.Boto3Error`, to ensure robust error management.\n",
    "4. **Logging**: Log responses and errors appropriately for monitoring and debugging purposes.\n",
    "5. **Kafka Cluster Configuration**: Ensure the Kafka cluster configuration (e.g., KafkaVersion, InstanceType) is correctly specified.\n",
    "6. **Sensitive Information Management**: Consider using environment variables or a secrets manager for handling sensitive information securely.\n",
    "7. **Region Verification**: Verify that the `region_name` is correct and matches the intended AWS region for deployment.\n",
    "8. **Broker Nodes**: Ensure the number of broker nodes is appropriate for the expected load and redundancy requirements. The number of broker nodes should be a multiple of the availability zones used in the subnet.\n",
    "9. **Mounting File Paths**: Add the different paths assigned for mounting files to ensure proper file system setup and access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9d1eb4-6d08-491e-aa77-48ead116be0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing the AWS Credentials and  Other details"
    }
   },
   "outputs": [],
   "source": [
    "# Define AWS configuration details in the aws_config dictionary\n",
    "# DBTITLE 1,Initialize Config Settings\n",
    "if 'config' not in locals() or not isinstance(config, dict):\n",
    "    config = {}\n",
    "\n",
    "config['aws'] = {\n",
    "    'access_key_id': 'AKIA3BYT2DKQQQ7QDXVW',\n",
    "    'secret_access_key': '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',\n",
    "    'region_name': 'us-west-2',\n",
    "    'subnets': [\n",
    "        'subnet-0682160c6cf0f7d33',  # SubnetID-1 \n",
    "        'subnet-0ef26ca8237f37fae'   # SubnetID-2\n",
    "    ],\n",
    "    'security_group': 'sg-098d6eca4e4744f3f',  # Security group ID\n",
    "    'cluster_name': 'real-time-pos-msk',  # Unique cluster name\n",
    "    'kafka_version': '2.8.1',\n",
    "    'number_of_broker_nodes': 4,\n",
    "    'instance_type': 'kafka.m5.large',\n",
    "    'cluster_arn': 'arn:aws:kafka:us-west-2:759713897121:cluster/real-time-pos-msk/d494f0b6-6c35-4d6e-ab46-60da511fcafd-11'\n",
    "}\n",
    "\n",
    "## Config Settings for DBFS Mount Point\n",
    "config['dbfs_mount_name'] = f'/mnt/real-time-pos/' \n",
    "\n",
    "# Store the filenames for the data files into Config\n",
    "config['inventory_change_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_store001.txt'\n",
    "config['inventory_change_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_online.txt'\n",
    " \n",
    "# snapshot data files\n",
    "config['inventory_snapshot_store001_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_store001.txt'\n",
    "config['inventory_snapshot_online_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_snapshot_online.txt'\n",
    " \n",
    "# static data files\n",
    "config['stores_filename'] = config['dbfs_mount_name'] + '/data-generator/store.txt'\n",
    "config['items_filename'] = config['dbfs_mount_name'] + '/data-generator/item.txt'\n",
    "config['change_types_filename'] = config['dbfs_mount_name'] + '/data-generator/inventory_change_type.txt'\n",
    "\n",
    "# Config Settings for Checkpoint Files\n",
    "config['inventory_snapshot_path'] = config['dbfs_mount_name'] + '/inventory_snapshots/'\n",
    "# Config Settings for DLT Data\n",
    "config['dlt_pipeline'] = config['dbfs_mount_name'] + '/dlt_pipeline_pos'\n",
    "\n",
    "# Identify Database for Data Objects and initialize it\n",
    "database_name = f'pos_dlt'\n",
    "config['database'] = database_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab9783f-7481-4caa-a227-b0211f906203",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing library"
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer\n",
    "from kafka.errors import NoBrokersAvailable, KafkaTimeoutError\n",
    "import boto3\n",
    "import socket\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e841d01d-3b44-405b-9dcb-d0d723f257d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating MSK Cluster Using Boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea927ba-ab9a-4581-b69f-78bb6ee7e336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Initialize the Kafka client with AWS credentials\n",
    "    client = boto3.client(\n",
    "        'kafka',\n",
    "        region_name=config['aws']['region_name'],\n",
    "        aws_access_key_id=config['aws']['access_key_id'],\n",
    "        aws_secret_access_key=config['aws']['secret_access_key']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create the Kafka cluster\n",
    "        response = client.create_cluster(\n",
    "            ClusterName=config['aws']['cluster_name'],\n",
    "            KafkaVersion=config['aws']['kafka_version'],\n",
    "            NumberOfBrokerNodes=config['aws']['number_of_broker_nodes'],\n",
    "            BrokerNodeGroupInfo={\n",
    "                'InstanceType': config['aws']['instance_type'],\n",
    "                'ClientSubnets': config['aws']['subnets'],\n",
    "                'SecurityGroups': [config['aws']['security_group']]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Retrieve and print the cluster ARN\n",
    "        cluster_arn = response['ClusterArn']\n",
    "        print(f\"Cluster ARN: {cluster_arn}\")\n",
    "\n",
    "    except client.exceptions.ConflictException:\n",
    "        # Handle the case where the cluster already exists\n",
    "        print(f\"Cluster {config['aws']['cluster_name']} already exists\")\n",
    "\n",
    "except NoCredentialsError:\n",
    "    # Handle the case where AWS credentials are not available\n",
    "    print(\"Credentials not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354797cd-7237-4b01-a64f-6bfbcf0ca4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Details for  MSK Cluster\n",
    "\n",
    "1. List all available MSK clusters in your AWS account.\n",
    "2. Retrieve detailed information about a specific MSK cluster, including its configuration and status.\n",
    "3. Monitor the health and performance metrics of the MSK cluster.\n",
    "4. Explore the topics and partitions within the MSK cluster.\n",
    "5. Check the connectivity and security settings of the MSK cluster.\n",
    "6. Review the broker nodes and their distribution across availability zones.\n",
    "7. Analyze the logs and events related to the MSK cluster for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1965853e-0b77-4b0c-a6db-fc2b812358c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def initialize_msk_session(config):\n",
    "    \"\"\"\n",
    "    Initialize a session using Amazon MSK with provided AWS credentials and region.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=config['aws']['access_key_id'],\n",
    "            aws_secret_access_key=config['aws']['secret_access_key'],\n",
    "            region_name=config['aws']['region_name']\n",
    "        )\n",
    "        return session\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not available.\")\n",
    "        return None\n",
    "\n",
    "def list_msk_clusters(msk_client):\n",
    "    \"\"\"\n",
    "    List all MSK clusters using the provided MSK client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        clusters = msk_client.list_clusters()\n",
    "        return clusters\n",
    "    except ClientError as e:\n",
    "        print(f\"Failed to list clusters: {e}\")\n",
    "        return None\n",
    "\n",
    "def describe_msk_cluster(msk_client, cluster_arn):\n",
    "    \"\"\"\n",
    "    Describe a specific MSK cluster using its ARN.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cluster_info = msk_client.describe_cluster(ClusterArn=cluster_arn)\n",
    "        return cluster_info\n",
    "    except ClientError as e:\n",
    "        print(f\"Failed to describe cluster: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize a session using Amazon MSK with configuration\n",
    "session = initialize_msk_session(config)\n",
    "\n",
    "if session:\n",
    "    # Create an MSK client\n",
    "    msk_client = session.client('kafka')\n",
    "\n",
    "    # List all clusters\n",
    "    clusters = list_msk_clusters(msk_client)\n",
    "\n",
    "    if clusters and clusters.get('ClusterInfoList'):\n",
    "        # Select a specific cluster (for example, the first one in the list)\n",
    "        selected_cluster_arn = clusters['ClusterInfoList'][0]['ClusterArn']\n",
    "        cluster_info = describe_msk_cluster(msk_client, selected_cluster_arn)\n",
    "        if cluster_info:\n",
    "            display(cluster_info)\n",
    "    else:\n",
    "        print(\"No clusters available or failed to retrieve clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b9835d-b259-449c-98b6-ffed858fa75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MSK Cluster Communication Ports\n",
    "\n",
    "- **Plaintext Communication**: Use port `9092` to communicate with brokers in plaintext.\n",
    "- **TLS Encryption**:\n",
    "  - Use port `9094` for access from within AWS.\n",
    "  - Use port `9194` for public access.\n",
    "- **SASL/SCRAM Authentication**:\n",
    "  - Use port `9096` for access from within AWS.\n",
    "  - Use port `9196` for public access.\n",
    "- **IAM Access Control**:\n",
    "  - Use port `9098` for access from within AWS.\n",
    "  - Use port `9198` for public access.\n",
    "- **Apache ZooKeeper Communication**:\n",
    "  - Use port `2182` for TLS encryption.\n",
    "  - Default port is `2181`.\n",
    "\n",
    "#### Example Commands\n",
    "```sh\n",
    "nc -vz b-1.realtimepos.qk3h2c.c11.kafka.us-west-2.amazonaws.com 9094\n",
    "nc -vz b-2.realtimepos.qk3h2c.c11.kafka.us-west-2.amazonaws.com 9094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b80c83af-e6cb-4a49-be21-8407161f469f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "# To communicate with brokers in plaintext, use port 9092.\n",
    "\n",
    "# To communicate with brokers with TLS encryption, use port 9094 for access from within AWS and port 9194 for public access.\n",
    "\n",
    "# To communicate with brokers with SASL/SCRAM, use port 9096 for access from within AWS and port 9196 for public access.\n",
    "\n",
    "# To communicate with brokers in a cluster that is set up to use IAM access control, use port 9098 for access from within AWS and port 9198 for public access.\n",
    "\n",
    "### IAM Authentication\n",
    "nc -vz b-1.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9098\n",
    "nc -vz b-2.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9098\n",
    "\n",
    "# SASL/SCRAM Authentication\n",
    "nc -vz b-1.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9096\n",
    "nc -vz b-2.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9096\n",
    "\n",
    "## Plaintext Authentication\n",
    "nc -vz b-1.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9092\n",
    "nc -vz b-2.realtimeposmsk.dhq8fv.c11.kafka.us-west-2.amazonaws.com 9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628f6468-5286-4c5b-99d4-3aec30e8f115",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing the Boto3 Session"
    }
   },
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id=config['aws']['access_key_id'],\n",
    "    aws_secret_access_key=config['aws']['secret_access_key'],\n",
    "    region_name=config['aws']['region_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424b3d8e-52e0-4d6e-8a21-d05d512f5ea5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing the Producer and Topics"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize a session using Amazon MSK\n",
    "# Create an MSK client\n",
    "msk_client = session.client('kafka')\n",
    "\n",
    "try:\n",
    "    # Get bootstrap brokers\n",
    "    response = msk_client.get_bootstrap_brokers(\n",
    "        ClusterArn=config['aws']['cluster_arn'],\n",
    "    )\n",
    "    bootstrap_servers = response['BootstrapBrokerString']\n",
    "except NoCredentialsError:\n",
    "    print(\"No credentials available. Please check your AWS credentials.\")\n",
    "\n",
    "# List all topics in the cluster\n",
    "try:\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        security_protocol='PLAINTEXT',\n",
    "        client_id=socket.gethostname(),\n",
    "    )\n",
    "    topics = admin_client.list_topics()\n",
    "    print(\"Topics in the cluster:\", topics)\n",
    "except NoBrokersAvailable:\n",
    "    print(\"No brokers available. Please check the broker endpoint and network connectivity.\")\n",
    "except KafkaTimeoutError:\n",
    "    print(\"Kafka timeout error. Failed to update metadata after 60.0 secs. Please check the broker endpoint and network connectivity.\")\n",
    "\n",
    "# Create a Kafka producer using PLAINTEXT connection\n",
    "try:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        security_protocol='PLAINTEXT',\n",
    "        client_id=socket.gethostname(),\n",
    "    )\n",
    "    print(\"Producer Initalized successfully.\")\n",
    "except NoBrokersAvailable:\n",
    "    print(\"No brokers available. Please check the broker endpoint and network connectivity.\")\n",
    "except KafkaTimeoutError:\n",
    "    print(\"Kafka timeout error. Failed to update metadata after 60.0 secs. Please check the broker endpoint and network connectivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40aeaa06-504c-484e-a102-cfd6450278ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create A S3 Bucket and Mount to DBFS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efe300a-4652-4713-b731-ea864a99ec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the S3 bucket and mount point\n",
    "s3_bucket = \"s3a://real-time-pos-msk/\"\n",
    "mount_point = config['dbfs_mount_name']\n",
    "\n",
    "# Set AWS credentials\n",
    "aws_access_key_id = config['aws']['access_key_id']\n",
    "aws_secret_access_key = config['aws']['secret_access_key']\n",
    "region = config['aws']['region_name']\n",
    "\n",
    "# Configure Spark to use the AWS credentials\n",
    "spark.conf.set(\"fs.s3a.access.key\", aws_access_key_id)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
    "\n",
    "# Check if the mount point already exists\n",
    "mounts = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "if mount_point in mounts:\n",
    "    print(\"It already exists\")\n",
    "else:\n",
    "    # Mount the S3 bucket\n",
    "    dbutils.fs.mount(\n",
    "      source=s3_bucket,\n",
    "      mount_point=mount_point,\n",
    "      extra_configs={\"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7917fc9-3d11-4b20-8b3b-e87a6a40f088",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking if Files are Mounted Or Not"
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/mnt/real-time-pos/data-generator"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7995473624292807,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Environment Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
