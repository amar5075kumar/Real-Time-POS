{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3dea42-e18b-48b7-95a1-a2c8161b2060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "This  Real-Time Point-of-Sale Solution utilizing Delta Live Tables (DLT) with Amazon Managed Workflows for Apache Managed Streaming Kafka(MSK). This solution showcases how Delta Live Tables can be utilized to construct a near real-time lakehouse architecture for calculating current inventories of various products across multiple store locations. Instead of directly transitioning from raw data ingestion to inventory calculations, I've structured this solution into two distinct phases.\n",
    "\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/pos_dlt_pipeline_UPDATED.png' width=800>\n",
    "\n",
    "## Introduction\n",
    "The initial phase, known as Bronze-to-Silver ETL, involves transforming ingested data to enhance accessibility. The actions performed on the data at this stage, such as breaking down nested arrays and removing duplicate records, do not involve applying any business-driven interpretations. The tables generated in this phase represent the Silver layer of our lakehouse architecture.\n",
    "\n",
    "In the subsequent phase, referred to as Silver-to-Gold ETL, the Silver tables are leveraged to derive the business-aligned output, which is the calculated current-state inventory. The resulting data is stored in a table representing the Gold layer of our architecture.\n",
    "\n",
    "Throughout this two-phase workflow, I employ Delta Live Tables (DLT) for orchestration and monitoring.\n",
    "\n",
    "## *Spark Structured Streaming vs. DLT(Delta Live Table)*\n",
    "\n",
    "### *Technology Stack*:\n",
    "\n",
    "Spark Structured Streaming is a component of Apache Spark that enables scalable, fault-tolerant stream processing with a familiar SQL-like interface.\n",
    "Delta Live Tables (DLT) is a higher-level abstraction built on top of Spark that provides orchestration, monitoring, and management capabilities for streaming workflows.\n",
    "\n",
    "### *Functionality*:\n",
    "Spark Structured Streaming focuses on stream processing and provides APIs for defining and executing streaming computations on data streams.\n",
    "DLT extends the capabilities of Spark Structured Streaming by offering features such as job scheduling, orchestration of workflows, and monitoring of streaming data pipelines.\n",
    "\n",
    "### *Ease of Use*:\n",
    "Spark Structured Streaming requires developers to write code to define streaming queries and manage the execution of those queries.\n",
    "DLT simplifies the development and management of streaming workflows by providing a higher-level interface for defining and orchestrating streaming jobs.\n",
    "\n",
    "### *Integration*:\n",
    "Spark Structured Streaming integrates seamlessly with the Apache Spark ecosystem and can leverage Spark's extensive library of connectors and processing functions.\n",
    "DLT integrates with Spark but adds additional functionality specific to managing streaming workflows, such as job scheduling and monitoring.\n",
    "Scalability:\n",
    "\n",
    "Both Spark Structured Streaming and DLT are designed for scalability and can handle large-scale streaming data processing tasks.\n",
    "DLT's additional management features can help optimize the scalability and performance of streaming workflows.\n",
    "\n",
    "By incorporating Delta Live Tables (DLT), the implementation of the streaming workflows remains consistent. DLT acts as a wrapper around our workflows, enabling orchestration, monitoring, and other enhancements that would otherwise require additional implementation efforts. In this context, DLT complements Spark Structured Streaming rather than replacing it. \n",
    "\n",
    "## Definition\n",
    "\n",
    "* *Notebook_01: Env Setup*\n",
    "* *Notebook_02: Generating_Raw_Data*\n",
    "* *Notebook_03: ETL for Broze Layer to Silver Layer*\n",
    "* *Notebook_04: ETL for Silver to Gold Layer*\n",
    "* *Notebook_05: Orchestration- JOB Scheduling_DLT*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf7e90a-8b4d-42b5-9a82-f55cd7ec5a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Readme",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
